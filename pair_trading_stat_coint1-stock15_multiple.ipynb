{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SparkSession, functions, types, Window\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "#assert sys.version_info >= (3, 5) # make sure we have Python 3.5+\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove null/nan or empty cell \n",
    "def to_null(c):\n",
    "    return functions.when(~(functions.col(c).isNull() | functions.isnan(functions.col(c)) | (functions.trim(functions.col(c)) == \"\")), functions.col(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean inpou data and pivot table \n",
    "def clean(minutedt):\n",
    "    minutedt=minutedt.select([to_null(c).alias(c) for c in minutedt.columns]).na.drop()\n",
    "\n",
    "    minutedt.createOrReplaceTempView('minutedt')\n",
    "\n",
    "    notnull = spark.sql(\"\"\" SELECT DateTime, SUBSTRING_INDEX(windcode, '.',1) AS symbol,close FROM minutedt \"\"\")\n",
    "    notnull.createOrReplaceTempView('notnull')\n",
    "    \n",
    "    pivoted = spark.sql(\"\"\"SELECT * FROM \n",
    "                    ( \n",
    "                    SELECT DateTime, symbol, close\n",
    "                    FROM notnull\n",
    "                    ) \n",
    "                    PIVOT \n",
    "                    (\n",
    "                    SUM(close)\n",
    "                    FOR symbol in (\"000008\" AS a, \"000009\" AS b, '000010' AS c, '000011' AS d, '000012' AS e, \"000014\" AS f, \"000016\" AS g, '000017' AS h, '000020' AS i, '000021' AS j)                    \n",
    "                    ) \n",
    "                    ORDER BY DateTime \"\"\").cache()\n",
    "    pivoted.createOrReplaceTempView('pivoted')\n",
    "\n",
    "    dt = spark.sql(\"\"\"SELECT DateTime, a,b,c,d,e,f,g,h ,i,j FROM pivoted \"\"\")\n",
    "    dt=dt.select([to_null(c).alias(c) for c in dt.columns]).na.drop()\n",
    "    dt.createOrReplaceTempView('dt')\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all cointgration stock pair under threshodl, pvalue <0.02\n",
    "def find_coint(regdt):\n",
    "    pvalue ={}\n",
    "    count =0\n",
    "    for i in regdt.columns:\n",
    "        count +=1\n",
    "        for j in regdt.columns[count:]:\n",
    "            if i != j:\n",
    "    #calculate slope, intercept using 2 year(2018-2019) window between two underlyers to determine their relationship\n",
    "                logdt = spark.sql(\"\"\" SELECT log({0}) AS y,log({1}) AS x FROM regdt\"\"\".format(i,j))\n",
    "                logdt.createOrReplaceTempView('logdt')\n",
    "                vectorAssembler = VectorAssembler(inputCols=['x'], outputCol=\"features\")\n",
    "\n",
    "                lr = LinearRegression(featuresCol='features', labelCol= 'y')\n",
    "\n",
    "                stages = [vectorAssembler, lr]  \n",
    "\n",
    "                pipeline = Pipeline(stages=stages)\n",
    "                model = pipeline.fit(logdt)\n",
    "\n",
    "                slope = model.stages[-1].coefficients[0]\n",
    "                intercept = model.stages[-1].intercept\n",
    "\n",
    "                spddt = spark.sql(\"\"\" SELECT (y - {0}* x - {1} )AS spread FROM logdt\"\"\".format(slope,intercept))\n",
    "                spddt.createOrReplaceTempView('spddt')\n",
    "                spddt = spddt.toPandas()\n",
    "                adfSpread = adfuller(spddt['spread'] , autolag ='AIC')\n",
    "                adf_pvalue = adfSpread[1]\n",
    "                print(i,j,'slope: ',slope,'intercept: ', intercept,'pvalue: ', adf_pvalue)\n",
    "                pvalue[(i,j)] = (adf_pvalue, intercept, slope)\n",
    "    \n",
    "    #find pairs with pvalue under threshold 0.005           \n",
    "    min_pvalue = 0.005\n",
    "    pv =0\n",
    "    slope =0\n",
    "    intercept = 0\n",
    "    pair1 = 0\n",
    "    pair2 = 0\n",
    "    result=[]\n",
    "    for k,v in pvalue.items():\n",
    "        if v[0] < min_pvalue:\n",
    "            pv= v[0]\n",
    "            slope = v[2]\n",
    "            intercept = v[1]\n",
    "            pair1 = k[0]\n",
    "            pair2 = k[1]      \n",
    "            result.append([pair1,pair2, pv, slope, intercept])   \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(coint_pair):\n",
    "    #get all valid pairs and trade each pair\n",
    "    for i in range(len(coint_pair)):\n",
    "        pair1 = coint_pair[i][0]\n",
    "        pair2 = coint_pair[i][1]\n",
    "        slope = coint_pair[i][3]\n",
    "        intercept = coint_pair[i][4]\n",
    "        \n",
    "        #calculate spread of the train window for each valid pair\n",
    "        dt2 = spark.sql(\"\"\" SELECT DateTime, {0}, {1}, (log({0}) - log({1}) * {2} - {3}) AS spread FROM train \"\"\".format(pair1,pair2,slope,intercept))\n",
    "        dt2.createOrReplaceTempView('dt2')\n",
    "        mu = dt2.agg(functions.avg(dt2['spread'])).collect()[0][0]\n",
    "        std = dt2.agg(functions.stddev(dt2['spread'])).collect()[0][0]\n",
    "        #set trading signal boundries if the spread is 2 times standard deviation from spread mean\n",
    "        up = mu +std*1.8\n",
    "        down = mu - std*1.8\n",
    "        #set stop_loss point\n",
    "        upsl = mu +std*2.8\n",
    "        downsl = mu - std*2.8\n",
    "        #set take profit point\n",
    "        uptp = mu +std*0.2\n",
    "        downtp = mu - std*0.2\n",
    "        \n",
    "        #simulate trade\n",
    "        test1 = spark.sql(\"\"\" SELECT DateTime, {0}, {1}, (log({0}) - log({1}) * {2} - {3}) AS spread FROM test \"\"\".format(pair1,pair2,slope,intercept))\n",
    "        test1.createOrReplaceTempView('test1')\n",
    "        \n",
    "        test1.toPandas()\n",
    "        \n",
    "        test3 = trade(test1,pair1,pair2, up, down, upsl,downsl,uptp,downtp, mu, slope)\n",
    "        #save trading history/result to csv\n",
    "        test3.to_csv('retcsv_{}_{}.csv'.format(pair1,pair2), index = True) \n",
    "    \n",
    "    return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trade simulation\n",
    "def trade(test1,pair1,pair2, up, down, upsl,downsl,uptp,downtp, mu ,slope):\n",
    "#use up/down boundries to determine trade position in test dataset (2020)\n",
    "    test1 = test1.withColumn(\"up\", functions.lit(up) )\n",
    "    test1 = test1.withColumn(\"down\",functions.lit( down) )\n",
    "    test1 = test1.withColumn(\"upsl\",functions.lit( upsl) )\n",
    "    test1 = test1.withColumn(\"downsl\", functions.lit(downsl) )\n",
    "    test1 = test1.withColumn(\"uptp\", functions.lit(uptp) )\n",
    "    test1 = test1.withColumn(\"downtp\", functions.lit(downtp) )\n",
    "    test1 = test1.withColumn(\"mu\",functions.lit( mu) )\n",
    "    test1.createOrReplaceTempView('test1')\n",
    "    \n",
    "    #set trade positions based on signal\n",
    "    test1 = spark.sql(\"\"\" SELECT DateTime, {0}, {1}, spread,\n",
    "                CASE \n",
    "                    WHEN spread >=upsl THEN 3\n",
    "                    WHEN spread >=up AND spread <upsl THEN 2\n",
    "                    WHEN spread >=uptp AND spread <up THEN 1\n",
    "                    WHEN spread < downsl THEN -3\n",
    "                    WHEN spread < down AND spread >=downsl THEN -2\n",
    "                    WHEN spread < downtp AND spread >=down THEN -1\n",
    "                ELSE 0\n",
    "                END AS level FROM test1 \"\"\" .format(pair1,pair2))\n",
    "\n",
    "    test1.createOrReplaceTempView('test1')\n",
    "\n",
    "    test1 = test1.withColumn('position',functions.lit(0) )\n",
    "    test1 = test1.withColumn('signal',functions.lit(0) )\n",
    "\n",
    "    test1 = test1.toPandas()\n",
    "    \n",
    "    #set trade signal\n",
    "    for i in range(1, len(test1)):\n",
    "        if test1['level'][i-1] ==1 and test1['level'][i] ==2:      #open position -> short I, long J\n",
    "            test1.loc[i,'signal'] = -2\n",
    "        elif test1['level'][i-1] ==1 and test1['level'][i] ==0:      #take profit -I+J -> long I, short J\n",
    "            test1.loc[i,'signal'] = 2 \n",
    "        elif test1['level'][i-1] ==2 and test1['level'][i] ==3:      #stop loss -I+J -> long I, short J\n",
    "            test1.loc[i,'signal'] = 3 \n",
    "        elif test1['level'][i-1] == -1 and test1['level'][i] == -2:      #open position -> long I, short J\n",
    "            test1.loc[i,'signal'] = 1 \n",
    "        elif test1['level'][i-1] == -1 and test1['level'][i] ==0:      #take profit +I-J -> short I, long J\n",
    "            test1.loc[i,'signal'] = -1\n",
    "        elif test1['level'][i-1] ==-2 and test1['level'][i] ==-3:      #stop loss +I-J -> short I, long J\n",
    "            test1.loc[i,'signal'] = -3 \n",
    "            \n",
    "     #set trade position \n",
    "    for i in range(1,len(test1)):\n",
    "        test1.loc[i,'position'] = test1['position'][i-1]\n",
    "        if test1['signal'][i] == 1:\n",
    "            test1.loc[i,'position'] = 1\n",
    "        elif test1['signal'][i] == -2:\n",
    "            test1.loc[i,'position'] = -1\n",
    "        elif test1['signal'][i] == -1 and test1['position'][i-1] == 1:\n",
    "            test1.loc[i,'position'] = 0\n",
    "        elif test1['signal'][i] == 2 and test1['position'][i-1] == -1:\n",
    "            test1.loc[i,'position'] = 0\n",
    "        elif test1['signal'][i] == 3:\n",
    "            test1.loc[i,'position'] = 0\n",
    "        elif test1['signal'][i] == -3:\n",
    "            test1.loc[i,'position'] = 0    \n",
    "           \n",
    "    #initialize portfolio\n",
    "    size=1000\n",
    "    test1['pair1_share'] = test1['position']*size\n",
    "    test1['pair2_share'] = round( - test1['pair1_share'] *slope *  test1[pair1]/ test1[pair2] )\n",
    "    test1['cash'] = 5000\n",
    "    \n",
    "    test3 = test1\n",
    "    for i in range(1,len(test3)):\n",
    "        test3.loc[i,'pair2_share'] = test3['pair2_share'][i-1]\n",
    "        test3.loc[i,'cash'] = test3['cash'][i-1] \n",
    "        if test3['position'][i-1] == 0 and test3['position'][i]  ==1:\n",
    "            test3.loc[i,'pair2_share'] = round( - test3['pair1_share'][i] *slope *  test3[pair1][i]/ test3[pair2][i] )\n",
    "            test3.loc[i, 'cash'] =  test3['cash'][i-1] - (test3['pair1_share'][i] *  test3[pair1][i] + test3['pair2_share'][i] *  test3[pair2][i] )\n",
    "        elif test3['position'][i-1] == 0 and test3['position'][i]  == -1:\n",
    "            test3.loc[i,'pair2_share'] = round( - test3['pair1_share'][i] *slope *  test3[pair1][i]/ test3[pair2][i] )\n",
    "            test3.loc[i, 'cash'] =  test3['cash'][i-1] - (test3['pair1_share'][i] *  test3[pair1][i] + test3['pair2_share'][i] *  test3[pair2][i] )\n",
    "        elif test3['position'][i-1] == 1 and test3['position'][i]  ==0:\n",
    "            test3.loc[i,'pair2_share'] = 0\n",
    "            test3.loc[i,'cash'] =  test3['cash'][i-1] + (test3['pair1_share'][i-1] *  test3[pair1][i] + test3['pair2_share'][i-1] *  test3[pair2][i] )\n",
    "        elif test3['position'][i-1] == -1 and test3['position'][i]  == 0:\n",
    "            test3.loc[i,'pair2_share'] = 0\n",
    "            test3.loc[i,'cash'] =  test3['cash'][i-1] + (test3['pair1_share'][i-1] *  test3[pair1][i] + test3['pair2_share'][i-1] *  test3[pair2][i] )\n",
    "\n",
    "    test3['asset'] = test3['cash']+ test3['pair1_share']*test3[pair1] +test3['pair2_share']*test3[pair2] \n",
    "    \n",
    "    return test3\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spark.sqlContext.implicits._\n",
    "def main():\n",
    "    # main logic starts here\n",
    "    inputs =  '15stock.csv'\n",
    "    minute_schema = types.StructType([\n",
    "        types.StructField('DateTime', types.StringType()),\n",
    "        types.StructField('windcode', types.StringType()),   \n",
    "        types.StructField('close', types.DoubleType()),\n",
    "\n",
    "    ])\n",
    "\n",
    "    minutedt = spark.read.csv(inputs, schema=minute_schema)\n",
    "    dt = clean(minutedt)\n",
    "    #training dataset\n",
    "    train = spark.sql(\"\"\" SELECT * FROM dt WHERE DateTime < '2019-10-01 08:00:00' \"\"\").cache()\n",
    "    train.createOrReplaceTempView('train')\n",
    "    #test dataset\n",
    "    test = spark.sql(\"\"\" SELECT * FROM dt WHERE DateTime > '2019-10-01 08:00:00'\"\"\").cache()\n",
    "    test.createOrReplaceTempView('test')\n",
    "    #select all time series other than time\n",
    "    #regdt = spark.sql(\"\"\" SELECT j,i,h,g,f,e,d,c,b,a FROM train\"\"\").cache()\n",
    "    regdt = spark.sql(\"\"\" SELECT j,i,h,g,f,e,d,c,b,a FROM train\"\"\").cache()\n",
    "    regdt.createOrReplaceTempView('regdt')\n",
    "    \n",
    "    coint_pair = find_coint(regdt)\n",
    "    for i in coint_pair:\n",
    "        training(coint_pair)\n",
    "        \n",
    "    return 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j i slope:  0.6497526517518525 intercept:  0.4368152439272683 pvalue:  0.6913730698746592\n",
      "j h slope:  1.0339348713179866 intercept:  0.4178605375480168 pvalue:  0.6400402787669912\n",
      "j g slope:  0.6259066028269915 intercept:  1.1022599099641026 pvalue:  0.6622326986950244\n",
      "j f slope:  0.7303258761524786 intercept:  0.331430279851079 pvalue:  0.8081818173753167\n",
      "j e slope:  0.34791444989088577 intercept:  1.4809087623362143 pvalue:  0.7420195558000938\n",
      "j d slope:  0.4458952108076766 intercept:  0.9701209677086502 pvalue:  0.7873277415581783\n",
      "j c slope:  -0.15915861766796438 intercept:  2.2724734223101097 pvalue:  0.6061277000298217\n",
      "j b slope:  0.7954815861680784 intercept:  0.7278908758374565 pvalue:  0.8972235131876087\n",
      "j a slope:  0.23001473728679137 intercept:  1.7041211065975062 pvalue:  0.6970746998635708\n",
      "i h slope:  0.7048537365501637 intercept:  1.3752956730648742 pvalue:  0.10438411402655601\n",
      "i g slope:  0.47429598782812754 intercept:  1.7693236945857425 pvalue:  0.008066218279761683\n",
      "i f slope:  0.9100877398487444 intercept:  0.342963394620667 pvalue:  0.0005765891682157282\n",
      "i e slope:  0.40770134358863375 intercept:  1.818104358711689 pvalue:  0.011094664960172994\n",
      "i d slope:  0.5307444951033555 intercept:  1.1995097238511785 pvalue:  0.03514857105655473\n",
      "i c slope:  0.32903761957546596 intercept:  2.044666650210942 pvalue:  0.12377899556526006\n",
      "i b slope:  0.5330096070659599 intercept:  1.6021533918532116 pvalue:  0.09453558879274204\n",
      "i a slope:  0.3416626899566848 intercept:  1.969325561381746 pvalue:  0.043607106146252225\n",
      "h g slope:  0.45546548030408796 intercept:  0.8903603585185178 pvalue:  0.006416754272484109\n",
      "h f slope:  0.6932524636052216 intercept:  -0.0526507754017153 pvalue:  0.01149520879355358\n",
      "h e slope:  0.37270906493771433 intercept:  0.9682924491463056 pvalue:  0.015425844347580844\n",
      "h d slope:  0.4718715656692597 intercept:  0.43523038019098287 pvalue:  0.029184980576820868\n",
      "h c slope:  0.20363395765263076 intercept:  1.3075291129073086 pvalue:  0.04759301347008837\n",
      "h b slope:  0.5619153084408554 intercept:  0.6462330930946721 pvalue:  0.02645408591753052\n",
      "h a slope:  0.34559187628198923 intercept:  1.0556553621875286 pvalue:  0.01505170805870906\n",
      "g f slope:  1.3134914730944665 intercept:  -1.5778764656872866 pvalue:  0.029070227648085423\n",
      "g e slope:  0.7970539790483097 intercept:  0.2062325500127574 pvalue:  0.059175229837758234\n",
      "g d slope:  0.975932157655037 intercept:  -0.8529243862918271 pvalue:  0.07570662461706991\n",
      "g c slope:  0.5735505268248867 intercept:  0.7439591414933219 pvalue:  0.4775348414020745\n",
      "g b slope:  1.108088363331503 intercept:  -0.3262420690790789 pvalue:  0.23695771747543293\n",
      "g a slope:  0.6943355330736403 intercept:  0.46149632050720135 pvalue:  0.1914135752082745\n",
      "f e slope:  0.3589358184004769 intercept:  1.7680789769274234 pvalue:  0.002447937333450457\n",
      "f d slope:  0.4559587158301428 intercept:  1.2510021938535598 pvalue:  0.013548394247701741\n",
      "f c slope:  0.24901942351602296 intercept:  2.022832749509517 pvalue:  0.0488895665646638\n",
      "f b slope:  0.5026573753161067 intercept:  1.522189797007974 pvalue:  0.009706414569707254\n",
      "f a slope:  0.28925233519441174 intercept:  1.9188749370466558 pvalue:  0.0059809699607634255\n",
      "e d slope:  1.2089529611798921 intercept:  -1.291161098392496 pvalue:  0.0019112372024088975\n",
      "e c slope:  0.9012652011580834 intercept:  0.42760248482702684 pvalue:  0.3141609765503849\n",
      "e b slope:  1.1355018085771744 intercept:  -0.24275070762414896 pvalue:  0.5118496924431389\n",
      "e a slope:  0.919643822674818 intercept:  0.24602683721158827 pvalue:  0.0422215586821752\n",
      "d c slope:  0.7773605240159877 intercept:  1.3783624488089792 pvalue:  0.243438586377683\n",
      "d b slope:  0.8372562100650702 intercept:  1.0374874803366716 pvalue:  0.40007037439876225\n",
      "d a slope:  0.7130162553065126 intercept:  1.344453191852028 pvalue:  0.00860646743343907\n",
      "c b slope:  0.20404611390205069 intercept:  1.019093418021004 pvalue:  0.1266042294341721\n",
      "c a slope:  0.3728868562298761 intercept:  0.7892428872038469 pvalue:  0.0394262084375784\n",
      "b a slope:  0.40694086003357754 intercept:  1.0469965068377534 pvalue:  0.2674780217519848\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = SparkSession.builder.appName('example code').getOrCreate()\n",
    "    #assert spark.version >= '2.4' # make sure we have Spark 2.4+\n",
    "    spark.sparkContext.setLogLevel('WARN')\n",
    "    sc = spark.sparkContext\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
